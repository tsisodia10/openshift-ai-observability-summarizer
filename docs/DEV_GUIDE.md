# DEV_GUIDE.md - OpenShift AI Observability Summarizer

> **Comprehensive Development Guide for Human Developers & AI Assistants**  
> This file provides complete guidance for working with the AI Observability Summarizer project, combining development patterns, architecture, and comprehensive instructions for both **human developers** and **AI coding assistants**.

## üöÄ Project Overview

The **OpenShift AI Observability Summarizer** is an open source, CNCF-style project that provides advanced monitoring and automated summarization of AI model and OpenShift cluster metrics. It generates AI-powered insights and reports from Prometheus/Thanos metrics data.

### Key Capabilities
- **vLLM Monitoring**: GPU usage, latency, request volume analysis
- **OpenShift Fleet Monitoring**: Cluster-wide and namespace-scoped metrics
- **AI-Powered Insights**: LLM-based metric summarization and analysis
- **Report Generation**: HTML, PDF, and Markdown exports
- **Alerting & Notifications**: AI-powered alerts with Slack integration
- **Distributed Tracing**: OpenTelemetry and Tempo integration

## üìÅ Project Structure

```
summarizer/
‚îú‚îÄ‚îÄ src/                    # Main source code
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Core business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py      # Configuration management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py  # LLM communication
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py     # Metrics discovery & fetching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py    # Statistical analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reports.py     # Report generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ promql_service.py # PromQL generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ thanos_service.py # Thanos integration
‚îÇ   ‚îú‚îÄ‚îÄ ui/                # Streamlit UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui.py         # Multi-dashboard interface
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server/        # Model Context Protocol server
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.py         # MCP API implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py        # HTTP server entrypoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stdio_server.py # STDIO server for AI assistants
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools/         # MCP tools (observability_tools.py)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ integrations/  # AI assistant integration configs
‚îÇ   ‚îî‚îÄ‚îÄ alerting/          # Alerting service
‚îÇ       ‚îî‚îÄ‚îÄ alert_receiver.py # Alert handling
‚îú‚îÄ‚îÄ deploy/helm/           # Helm charts for deployment
‚îÇ   ‚îú‚îÄ‚îÄ mcp-server/        # MCP server Helm chart
‚îÇ   ‚îú‚îÄ‚îÄ ui/                # UI Helm chart
‚îÇ   ‚îî‚îÄ‚îÄ rag/               # RAG components (llama-stack, llm-service)
‚îú‚îÄ‚îÄ tests/                 # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ mcp/               # MCP server tests
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Core logic tests
‚îÇ   ‚îî‚îÄ‚îÄ alerting/          # Alerting tests
‚îú‚îÄ‚îÄ scripts/               # Development and deployment scripts
‚îî‚îÄ‚îÄ docs/                  # Documentation
```

## üîß Development Setup

### Prerequisites
- Python 3.11+
- `uv` package manager
- OpenShift CLI (`oc`)
- `helm` v3.x
- `yq` (YAML processor)
- Docker or Podman

### macOS: WeasyPrint for local PDF reports (optional)
WeasyPrint is used for generating PDF reports. Containers and CI handle dependencies automatically via `uv`, but for local macOS development you may need a system install:

```bash
brew install weasyprint
weasyprint --version

# If WeasyPrint cannot find libraries at runtime, set:
export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib:$DYLD_FALLBACK_LIBRARY_PATH
```


### Local Development
```bash
# Set up port-forwarding to cluster services
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE>

# If model is in different namespace:
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE> -m <MODEL_NAMESPACE>

# This script sets up:
# - Virtual environment activation (.venv)
# - Port-forwarding to Llamastack (localhost:8321)
# - Port-forwarding to Model service (localhost:8080)
# - Port-forwarding to Thanos (localhost:9090)
# - Metrics API (localhost:8000)
# - Streamlit UI (localhost:8501)

# Example:
./scripts/local-dev.sh -n default-ns
./scripts/local-dev.sh -n default-ns -m model-ns  # Model in different namespace
```

**Note**: The script automatically:
- Activates Python virtual environment if `.venv` exists
- Uses service-based port forwarding for better reliability
- Supports separate namespaces for different services

## üèóÔ∏è Architecture & Data Flow

### Core Components
1. **MCP Server** (`src/mcp_server/`): Model Context Protocol server for metrics analysis, report generation, and AI assistant integration
2. **UI** (`src/ui/ui.py`): Streamlit multi-dashboard frontend
3. **Core Logic** (`src/core/`): Business logic modules for metrics processing and LLM integration
4. **Alerting** (`src/alerting/`): Alert handling and Slack notifications
5. **Helm Charts** (`deploy/helm/`): OpenShift deployment configuration

### Data Flow
1. **Natural Language Question** ‚Üí PromQL generation via LLM
2. **PromQL Queries** ‚Üí Thanos/Prometheus for metrics data
3. **Metrics Data** ‚Üí Statistical analysis and anomaly detection
4. **Analysis Results** ‚Üí LLM summarization
5. **Summary** ‚Üí Report generation (HTML/PDF/Markdown)

### Key Services Integration
- **Prometheus/Thanos**: Metrics storage and querying
- **vLLM**: Model serving with /metrics endpoint
- **DCGM**: GPU monitoring metrics
- **Llama Stack**: LLM inference backend
- **OpenTelemetry/Tempo**: Distributed tracing

## üß™ Testing

### Test Commands
```bash
# Run all tests with coverage
uv run pytest -v --cov=src --cov-report=html --cov-report=term

# Run specific test categories
uv run pytest -v tests/mcp/           # MCP tests
uv run pytest -v tests/core/          # Core logic tests
uv run pytest -v tests/alerting/      # Alerting tests
uv run pytest -v tests/api/           # API tests

# Run specific test file
uv run pytest -v tests/mcp/test_api_endpoints.py

# View coverage report
open htmlcov/index.html
```

### Test Structure
- **`tests/mcp/`** - Metric Collection & Processing tests
- **`tests/core/`** - Core business logic tests
- **`tests/alerting/`** - Alerting service tests
- **`tests/api/`** - API endpoint tests

### Testing Strategy
- **Unit Tests**: Core business logic in `tests/core/`
- **Integration Tests**: API endpoints in `tests/mcp/`
- **Alert Tests**: Alerting functionality in `tests/alerting/`
- **Coverage**: Configured to exclude UI components and report assets

## üöÄ Building & Deployment

### Container Images
```bash
# Build all components
make build

# Build individual components
make build-ui            # Streamlit UI
make build-alerting      # Alerting service
make build-mcp-server    # MCP server

# Build with custom tag
make build TAG=v1.0.0
```

### Deployment
```bash
# Deploy to OpenShift (deploys default model)
make install NAMESPACE=your-namespace

# Deploy with alerting (deploys default model)
make install-with-alerts NAMESPACE=your-namespace

# Deploy with specific LLM model
make install NAMESPACE=your-namespace LLM=llama-3-2-1b-instruct

# Deploy with GPU tolerations (deploys default model)
make install NAMESPACE=your-namespace LLM_TOLERATION="nvidia.com/gpu"

# Deploy with safety models (deploys default model)
make install NAMESPACE=your-namespace SAFETY=llama-guard-3-8b

# Use existing model (specify LLM_URL as model service URL)
# Note: When LLM_URL is provided, HF_TOKEN will not be prompted since no new model deployment is needed

# URL with port (no processing applied):
make install NAMESPACE=your-namespace \
  LLM_URL=http://llama-3-2-3b-instruct-predictor.dev.svc.cluster.local:8080/v1

# URL without port (automatically adds :8080/v1):
make install NAMESPACE=your-namespace \
  LLM_URL=http://llama-3-2-3b-instruct-predictor.dev.svc.cluster.local

# Deploy individual components
make install-mcp-server NAMESPACE=your-namespace    # MCP server only
make install-metric-ui NAMESPACE=your-namespace     # UI only
```

### Observability Stack Management

The project includes a comprehensive observability stack with flexible deployment options:

#### **Complete Observability Stack**
```bash
# Install complete observability stack (MinIO + TempoStack + OTEL + tracing)
# Note: NAMESPACE is required for tracing setup
make install-observability-stack NAMESPACE=your-namespace

# Uninstall complete observability stack
# Note: NAMESPACE is required for tracing removal
make uninstall-observability-stack NAMESPACE=your-namespace
```

#### **Individual Observability Components**
```bash
# Install individual components
make install-minio                                           # MinIO storage only (uses observability-hub namespace)
make install-observability                                   # TempoStack + OTEL only (uses observability-hub namespace)
make setup-tracing NAMESPACE=your-namespace                 # Auto-instrumentation only (requires NAMESPACE)

# Uninstall individual components
make uninstall-minio                                         # MinIO storage only (uses observability-hub namespace)
make uninstall-observability                                 # TempoStack + OTEL only (uses observability-hub namespace)
make remove-tracing NAMESPACE=your-namespace                 # Auto-instrumentation only (requires NAMESPACE)
```

#### **NAMESPACE Requirements**
- **`install-observability-stack` / `uninstall-observability-stack`**: Require NAMESPACE for tracing components
- **`install-minio` / `uninstall-minio`**: Use hardcoded `observability-hub` namespace
- **`install-observability` / `uninstall-observability`**: Use hardcoded `observability-hub` namespace  
- **`setup-tracing` / `remove-tracing`**: Require NAMESPACE parameter

#### **MinIO Chart Simplification**
The MinIO chart has been simplified to use a single template file (`minio-simple.yaml`) that:
- Deploys MinIO as a StatefulSet with built-in bucket creation
- Uses MinIO's native `mc` client for bucket and user management
- Eliminates the need for separate initialization jobs
- Reduces complexity from 7 template files to 2 (including helpers)
- Provides more reliable and maintainable MinIO deployment

#### **Observability Stack Features**
- **MinIO**: S3-compatible object storage for trace data and log data persistence
- **TempoStack**: Multitenant trace storage and analysis with OpenShift integration
- **OpenTelemetry Collector**: Distributed tracing collection and forwarding
- **Auto-instrumentation**: Automatic Python application tracing setup
- **Dependency Management**: Proper installation/uninstallation order with dependency chains

### Management
```bash
# Check deployment status
make status NAMESPACE=your-namespace

# Uninstall
make uninstall NAMESPACE=your-namespace

# List available models
make list-models
```

## ‚öôÔ∏è Configuration

### Environment Variables
- `PROMETHEUS_URL`: Thanos/Prometheus endpoint (default: http://localhost:9090)
- `LLAMA_STACK_URL`: LLM backend URL (default: http://localhost:8321/v1/openai/v1)
- `LLM_API_TOKEN`: API token for LLM service
- `LLM_URL`: Use existing model URL (skips HF_TOKEN prompt and model deployment)
- `HF_TOKEN`: Hugging Face token (auto-prompted only when LLM_URL is not set)
- `MODEL_CONFIG`: JSON configuration for available models
- `THANOS_TOKEN`: Authentication token (default: reads from service account)
- `SLACK_WEBHOOK_URL`: Slack webhook for alerting notifications

### Model Configuration
Models are configured via `MODEL_CONFIG` environment variable as JSON:
```json
{
  "model-name": {
    "external": false,
    "url": "http://service:port",
    "apiToken": "token"
  }
}
```

### Available Models
```bash
# List available models for deployment
make list-models
```
Common models include:
- `llama-3-2-3b-instruct` (default)
- `llama-3-1-8b-instruct`
- `llama-3-3-70b-instruct`
- `llama-guard-3-8b` (safety model)

## üîç Common Development Patterns

### Adding New Metrics
1. Update metric discovery functions in `src/core/metrics.py`
2. Add PromQL queries for the new metrics
3. Update UI components to display the metrics
4. Add corresponding tests

### Adding New MCP Tools
1. Define request/response models in `src/core/models.py`
2. Implement business logic in appropriate `src/core/` module
3. Add MCP tool in `src/mcp_server/tools/`
4. Add corresponding tests

### Error Handling
- API endpoints use HTTPException for user-facing errors
- Internal errors are logged with stack traces
- LLM API key errors return specific user-friendly messages

## üöÄ Development Workflows

### 1. Feature Development
```bash
# 1. Set up local environment
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE>

# 2. Make changes to source code

# 3. Run tests
make test

# 4. Build and test locally
make build
```

### 2. Bug Fixing
```bash
# 1. Setup local environment
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE>

# 2. Activate virtual environment (for testing)
uv sync --group dev
source .venv/bin/activate

# 3. Run specific test
uv run pytest -v tests/core/test_specific_feature.py

# 4. Debug with coverage
uv run pytest -v --cov=src --cov-report=term-missing
```

### 3. Deployment Testing
```bash
# 1. Build images
make build TAG=test-$(date +%s)

# 2. Deploy to test namespace
make install NAMESPACE=test-namespace

# 3. Verify deployment
make status NAMESPACE=test-namespace

# 4. Test functionality
# Access UI via OpenShift route

# 5. Cleanup
make uninstall NAMESPACE=test-namespace
```

## üìä Monitoring & Debugging

### Setup Namespace
```bash
# Use the script with appropriate namespace parameters
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE>
# or with separate model namespace:
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE> -m <MODEL_NAMESPACE>
```

### Port Forwarding
```bash
# Manual port-forwarding (if script fails)
# Note: Replace <pod-name> with actual pod names from 'oc get pods'
oc port-forward pod/<thanos-pod-name> 9090:9090 -n openshift-monitoring &
oc port-forward pod/<llamastack-pod-name> 8321:8321 -n <DEFAULT_NAMESPACE> &
oc port-forward service/<model-service-name> 8080:8080 -n <MODEL_NAMESPACE> &
```

### Logs
```bash
# View pod logs (replace with your actual namespace)
oc logs -f deployment/metric-ui -n <DEFAULT_NAMESPACE>
oc logs -f deployment/mcp-server -n <DEFAULT_NAMESPACE>
oc logs -f deployment/metric-alerting -n <DEFAULT_NAMESPACE>
```

### Metrics
```bash
# Access MCP server health/metrics
oc port-forward svc/mcp-server 8085:8085 -n <DEFAULT_NAMESPACE>
# Then visit http://localhost:8085/health
```

## üõ†Ô∏è Useful Makefile Targets

### Development
- `./scripts/local-dev.sh -n <namespace>` - Set up local development environment
- `make test` - Run unit tests with coverage
- `make clean` - Clean up local images

### Building
- `make build` - Build all container images
- `make build-ui` - Build Streamlit UI
- `make build-alerting` - Build alerting service
- `make build-mcp-server` - Build MCP server

### Deployment
- `make install` - Deploy to OpenShift
- `make install-with-alerts` - Deploy with alerting
- `make install-mcp-server` - Deploy MCP server only
- `make install-metric-ui` - Deploy UI only
- `make status` - Check deployment status
- `make uninstall` - Remove deployment

### Observability Stack
- `make install-observability-stack` - Install complete observability stack
- `make uninstall-observability-stack` - Uninstall complete observability stack
- `make install-minio` - Install MinIO storage only
- `make uninstall-minio` - Uninstall MinIO storage only
- `make install-observability` - Install TempoStack + OTEL only
- `make uninstall-observability` - Uninstall TempoStack + OTEL only
- `make setup-tracing` - Enable auto-instrumentation
- `make remove-tracing` - Disable auto-instrumentation

### Configuration
- `make config` - Show current configuration
- `make list-models` - List available LLM models
- `make help` - Show all available targets

## üîß Troubleshooting

### Common Issues

#### Port Forwarding Fails
```bash
# Check if pods are running
oc get pods -n <DEFAULT_NAMESPACE>

# Restart port-forwarding
./scripts/local-dev.sh -n <DEFAULT_NAMESPACE>
```

#### Tests Fail
```bash
# Ensure virtual environment is activated
source .venv/bin/activate

# Reinstall dependencies
uv sync --group dev --reinstall

# Run tests with verbose output
uv run pytest -v --tb=short
```

#### Build Fails
```bash
# Check Docker/Podman is running
docker ps

# Clean and rebuild
make clean
make build
```

#### Deployment Issues
```bash
# Check namespace exists
oc get namespace <DEFAULT_NAMESPACE>

# Check Helm releases
helm list -n <DEFAULT_NAMESPACE>

# View deployment events
oc get events -n <DEFAULT_NAMESPACE> --sort-by='.lastTimestamp'
```

## üîí Security Considerations
- Service account tokens are read from mounted volumes
- SSL verification uses cluster CA bundle when available
- No secrets should be logged or committed to repository
- API endpoints use proper authentication and authorization

## üîê Credential Management

### Required Credentials
- **Quay Registry**: Username and password for container image pushing
- **OpenShift**: Server URL and authentication token
- **Hugging Face**: API token for model access

### Retrieving Credentials

#### Quay Registry Access
- **Search for**: `Appeng Quay ecosystem-appeng+aiobs kubernetes secret and token` in Bitwarden
  - **Required fields**: Username and Password
- **Purpose**: Pushing container images to Quay registry

#### OpenShift Access
- **For GitHub Actions workflows**: Run `./scripts/ocp-setup.sh -s -n <namespace>` to generate the required token
- **Manual setup**: Access your password manager and search for:
  - `openshift-ai-observability-summarizer ai-kickstart (aiobs)` for OCP server user/password
  - **Required fields**: Username and Password
- **Purpose**: Deploying to OpenShift clusters

#### Hugging Face Access
- User your personal "Hugging Face" token (_read token is sufficient_)
- **Purpose**: Accessing AI models and datasets

### Security Best Practices
- Never commit credentials to source code
- Use GitHub repository secrets for CI/CD
- Rotate credentials regularly
- Limit credential scope to minimum required permissions

## üìö Additional Resources

- **README.md** - Comprehensive project overview and setup
- **docs/GITHUB_ACTIONS.md** - CI/CD workflow documentation
- **docs/SEMANTIC_VERSIONING.md** - Version management guidelines

## üéØ Quick Reference

### File Locations
- **MCP Server**: `src/mcp_server/main.py`
- **Core Logic**: `src/core/llm_summary_service.py`
- **UI**: `src/ui/ui.py`
- **Tests**: `tests/`
- **Helm Charts**: `deploy/helm/`

### Key Commands
- **Local Dev**: `./scripts/local-dev.sh -n <namespace>`
- **Tests**: `uv run pytest -v`
- **Build**: `make build`
- **Deploy**: `make install NAMESPACE=ns`
- **Status**: `make status NAMESPACE=ns`

### Environment Variables
- `REGISTRY` - Container registry (default: quay.io)
- `VERSION` - Image version (default: 0.1.2)
- `LLM` - LLM model ID for deployment
- `PROMETHEUS_URL` - Metrics endpoint
- `LLAMA_STACK_URL` - LLM backend URL

---

**üí° Tip**: Use `make help` to see all available Makefile targets and their descriptions.
