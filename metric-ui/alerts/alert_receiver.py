import requests
import json
import os
import datetime
from llama_stack_client import LlamaStackClient

# --- CONFIG ---
ALERTMANAGER_URL = os.getenv("ALERTMANAGER_URL", "http://localhost:9093")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
LLAMA_STACK_URL = os.getenv("LLAMA_STACK_URL", "http://localhost:8321")
TIME_WINDOW = int(os.getenv("TIME_WINDOW", 60))

# Handle token input from volume or literal
token_input = os.getenv("AUTH_TOKEN", "/var/run/secrets/kubernetes.io/serviceaccount/token")
if os.path.exists(token_input):
    with open(token_input, "r") as f:
        AUTH_TOKEN = f.read().strip()
else:
    AUTH_TOKEN = token_input

# CA bundle location (mounted via ConfigMap)
CA_BUNDLE_PATH = "/etc/pki/ca-trust/extracted/pem/ca-bundle.crt"
verify = CA_BUNDLE_PATH if os.path.exists(CA_BUNDLE_PATH) else True

# --- LLAMASTACK SETUP ---
client = LlamaStackClient(base_url=LLAMA_STACK_URL)
llm = next(m for m in client.models.list() if m.model_type == "llm")

# pull active alerts from Alertmanager
def get_active_alerts() -> dict:
    headers = {"Authorization": f"Bearer {AUTH_TOKEN}"}
    endpoint = f"{ALERTMANAGER_URL}/api/v2/alerts"
    verify = CA_BUNDLE_PATH if os.path.exists(CA_BUNDLE_PATH) else True
    try:
        response = requests.get(
            endpoint,
            headers=headers,
            verify=verify,
        )
        response.raise_for_status()  
        print("Alerts successfully retrieved from Alertmanager")
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error querying Alertmanager: {e}")
        return None

def send_slack_message(payload):
    headers = {"Content-Type": "application/json"}
    if SLACK_WEBHOOK_URL == "":
        print("No Slack URL found")
        return False
    try:
        response = requests.post(SLACK_WEBHOOK_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        print("Slack message sent successfully!")
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error sending Slack message: {e}")
        return False
    
# generate a description based on the alert labels
def generate_description(labels):
    labels = json.dumps(labels)
    prompt = """
    You are an AI assistant designed to generate concise, informative, and *technically detailed* Slack message descriptions for OpenShift vLLM alerts. Your task is to analyze the provided alert data, *especially the 'expr' field*, and create a clear, actionable description of the problem.

    Provide only the description text. Start the description with "This alert..." or "This alert indicates..." to briefly summarize the general nature of the alert. Then, use a Markdown bulleted list to detail the following points:
    1.  Interpret the expression to explain what the issue is in understandable English. Do NOT mention "Prometheus Query Language" or include the raw expression string in your explanation. Instead, describe the metric and threshold being monitored.
    2.  **Affected components:** Mention the model_name, pod, namespace, and service to clearly identify what is impacted. This should be in a clear bulleted list.
    3.  Provide initial troubleshooting suggestions or common solutions to resolve the issue in a single sentence.

    Do not add any prefixes like 'ALERT:' or 'Severity:' or a separate summary line. The output should be ready to be embedded directly into a Slack message.

    Here is the alert data: 
    """
    response = client.inference.chat_completion(
        model_id=llm.identifier,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": labels},
        ],
        stream=False
    )

    return response.completion_message.content
    
# formats slack message for a single alert
def format_slack_message(alert_data):
    alertname = alert_data['labels'].get('alertname', 'N/A')
    severity = alert_data['labels'].get('severity', 'info').upper()
    generator_url = alert_data.get('generatorURL', 'No generator URL.')
    starts_at_iso = alert_data.get('startsAt')

    description = generate_description(alert_data['labels'])

    # get firing start time in UTC
    starts_at_iso = alert_data.get('startsAt')
    starts_at_formatted = "N/A"
    if starts_at_iso:
        try:
            dt_object_utc = datetime.datetime.fromisoformat(starts_at_iso.replace('Z', '+00:00'))
            starts_at_formatted = dt_object_utc.strftime('%Y-%m-%d %H:%M:%S UTC')
        except ValueError:
            pass

    color_emoji = ""
    if severity == "CRITICAL":
        color_emoji = ":red_circle: "
    elif severity == "WARNING":
        color_emoji = ":large_orange_circle: "
    elif severity == "INFO":
        color_emoji = ":large_blue_circle: "

    message_text = (
        f"{color_emoji}*ALERT: {alertname} [{severity}]*\n\n"
        f"{description}\n\n"
        f"Started At: {starts_at_formatted}\n"
        f"<{generator_url}|View on console>"
    )

    payload = {
        "text": message_text,
        "mrkdwn": True
    }

    return payload

# filters Alertmanager alerts by name + time and sends a slack message for each resulting alert
def process_vllm_alerts_and_notify(alerts, time_window=TIME_WINDOW):
    if alerts:
        now_utc = datetime.datetime.now(datetime.timezone.utc) 
        time_threshold = now_utc - datetime.timedelta(seconds=time_window)

        found = False
        for alert in alerts:
            alertname = alert['labels'].get('alertname', '')
            starts_at_iso = alert.get('startsAt')
            test_alert_label = alert['labels'].get('test_alert', 'false').lower()

            # filter by VLLM alerts
            if alertname.startswith("VLLM") and test_alert_label != 'true':
                
                if starts_at_iso:
                    try:
                        # filter by time, only notify on alerts that started within given time window
                        alert_start_time_utc = datetime.datetime.fromisoformat(starts_at_iso.replace('Z', '+00:00'))
                        if alert_start_time_utc >= time_threshold:
                            print(f"\n--- Found NEW relevant VLLM Alert: {alertname} ---")
                            slack_payload = format_slack_message(alert)
                            send_slack_message(slack_payload)
                            found = True
                        else:
                            print(f"VLLM alert '{alertname}' is active but started too long ago ({alert_start_time_utc}). Skipping.")
                    except ValueError:
                        print(f"Warning: Could not parse startsAt time for alert '{alertname}': {starts_at_iso}")
        if not found:
            print("No new alerts found")
    else:
        print("No alerts to process")

def generate_test():
    alert = json.loads('{"alertname": "VLLMHighAverageInferenceTime", "container": "kserve-container", "endpoint": "vllm-serving-runtime-metrics", "engine": "0", "expr": "rate(vllm:request_inference_time_seconds_sum[5m]) / rate(vllm:request_inference_time_seconds_count[5m]) > 2", "for": "5m", "instance": "10.129.2.73:8080", "job": "llama-3-2-3b-instruct-metrics", "model_name": "meta-llama/Llama-3.2-3B-Instruct", "namespace": "m3", "pod": "llama-3-2-3b-instruct-predictor-9fd74489-c6dwt", "prometheus": "openshift-user-workload-monitoring/user-workload", "service": "llama-3-2-3b-instruct-metrics", "severity": "warning"}')
    desc = generate_description(alert)
    print(desc)

def main():
    alerts = get_active_alerts()
    process_vllm_alerts_and_notify(alerts)

if __name__ == "__main__":
    main()
    # generate_test()